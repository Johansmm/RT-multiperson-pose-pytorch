{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "pose_results.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "05eadac4b19e4315a7ca29097abd76d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VideoModel",
          "state": {
            "_view_name": "VideoView",
            "_dom_classes": [],
            "_model_name": "VideoModel",
            "format": "mp4",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "controls": true,
            "_view_count": null,
            "width": "",
            "_view_module_version": "1.5.0",
            "layout": "IPY_MODEL_f86a523a575b4ec6b86cd6097c5a48ee",
            "height": "",
            "_model_module": "@jupyter-widgets/controls",
            "loop": true,
            "autoplay": true
          }
        },
        "f86a523a575b4ec6b86cd6097c5a48ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrjZTtw2rMCf"
      },
      "source": [
        "# Deep Learning project\n",
        "Intro by Tatiana"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiaLI5xU4EeM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8810c374-ca0d-4da8-b8f2-f95dba6579fb"
      },
      "source": [
        "# Libraries\n",
        "# Colab libraries\n",
        "from google.colab import drive\n",
        "from google.colab import output\n",
        "drive.mount('/content/gdrive')\n",
        "colab_path = \"/content/gdrive/My Drive/Colab Notebooks/\"\n",
        "\n",
        "# Basis libraries\n",
        "import os, re, sys, math, time, scipy, argparse, copy\n",
        "import cv2, matplotlib, json\n",
        "import numpy as np\n",
        "import matplotlib.gridspec as gridspec\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from ipywidgets import Video\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import torchvision\n",
        "from torchsummary import summary\n",
        "from collections import OrderedDict\n",
        "from scipy.ndimage.morphology import generate_binary_structure\n",
        "from scipy.ndimage.filters import gaussian_filter, maximum_filter"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1b25TcW4ik9"
      },
      "source": [
        "## Section 0: Download and install repository\n",
        "First, we will download the repository that we copied from the [original repository](https://github.com/tensorboy/pytorch_Realtime_Multi-Person_Pose_Estimation) in order to make some changes for educational purposes. Then, we install libraries and some dependences explained in the original repository."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKtbnvkHrBfA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8b6eb55-3ea7-4382-9567-3a9cf095d913"
      },
      "source": [
        "# Independence install\n",
        "!sudo apt-get install swig\n",
        "%cd $colab_path\n",
        "if not os.path.isdir(\"RT-multiperson-pose-pytorch\"):\n",
        "  # Repository clone\n",
        "  !git clone https://github.com/Johansmm/RT-multiperson-pose-pytorch.git\n",
        "  %cd \"RT-multiperson-pose-pytorch\"\n",
        "  %cd lib/pafprocess \n",
        "  # Repository compile\n",
        "  !sh make.sh\n",
        "\n",
        "# Libraries install\n",
        "%cd $colab_path\"/RT-multiperson-pose-pytorch\"\n",
        "!python -m pip install -r ./requirements.txt\n",
        "!git submodule init && git submodule update\n",
        "# Weights download\n",
        "if not os.path.isfile(\"./pose_model.pth\"):\n",
        "  !wget https://www.dropbox.com/s/ae071mfm2qoyc8v/pose_model.pth\n",
        "output.clear()\n",
        "print(\"[INFO]: Proyect uploaded successfully\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO]: Proyect uploaded successfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXqO9ha9lQao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52f51ae0-bd27-47c0-f077-1dd396aa5cbb"
      },
      "source": [
        "!python demo/picture_demo.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bulding VGG19\n",
            "Done !\n",
            "0.5459940652818991\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNWcHBWR6K7n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64b0e254-90e1-46ed-ea16-f96d41755330"
      },
      "source": [
        "# Framework libraries\n",
        "%cd $colab_path\"/RT-multiperson-pose-pytorch\"\n",
        "sys.path.append('.')\n",
        "from lib.network.rtpose_vgg import get_model\n",
        "from lib.network import im_transform\n",
        "from lib.utils.common import Human, BodyPart, CocoPart, CocoColors, CocoPairsRender, draw_humans\n",
        "from lib.utils.paf_to_pose import paf_to_pose_cpp\n",
        "from lib.config import cfg, update_config\n",
        "from evaluate.coco_eval import get_outputs, handle_paf_and_heat, run_eval"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks/RT-multiperson-pose-pytorch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "174xVRPR9QN-"
      },
      "source": [
        "## Section 2: Replication of results\n",
        "In this section we replicated some resutls. First, we need download the data. For this case, we will use the `sh` compiler provided by [original repository](https://github.com/tensorboy/pytorch_Realtime_Multi-Person_Pose_Estimation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uBTojMrmtud",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21782144-f99f-4606-f8c8-93da959f4673"
      },
      "source": [
        "%cd $colab_path\"/RT-multiperson-pose-pytorch\"\n",
        "data_download = False # For download COCO dataset\n",
        "if data_download and not os.path.isdir(\"data/coco\"):\n",
        "  !mkdir data\n",
        "  %cd data\n",
        "  !sh ../lib/datasets/CocoDataDownloader.sh\n",
        "  %cd $colab_path\"/RT-multiperson-pose-pytorch\"\n",
        "  output.clear()\n",
        "  print(\"[INFO]: Coco database downloaded successfully\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks/RT-multiperson-pose-pytorch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MekeFPVJ2gKJ"
      },
      "source": [
        "Now, we defined some principal functions and the neuronal network architecture.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78deQRcV2mOz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04ba3983-7b68-42b4-ab51-6d2af0959e0c"
      },
      "source": [
        "%cd $colab_path\"/RT-multiperson-pose-pytorch\"\n",
        "class Namespace:\n",
        "  def __init__(self, **kwargs):\n",
        "    self.__dict__.update(kwargs)\n",
        "\n",
        "def load_rtpose_model():\n",
        "  args = Namespace(cfg = './experiments/vgg19_368x368_sgd.yaml', weight = 'pose_model.pth', opts = [])\n",
        "  update_config(cfg, args)\n",
        "  model = get_model('vgg19')     \n",
        "  model.load_state_dict(torch.load(args.weight))\n",
        "  model = torch.nn.DataParallel(model).cuda()\n",
        "  model.float()\n",
        "  model.eval()\n",
        "  return model\n",
        "\n",
        "def im_forward(image, model):\n",
        "  with torch.no_grad():\n",
        "    paf, heatmap, im_scale = get_outputs(image, model, 'rtpose')\n",
        "  return paf, heatmap, im_scale\n",
        "\n",
        "def human_forward(image, model):\n",
        "  paf, heatmap, im_scale = im_forward(image, model)\n",
        "  humans = paf_to_pose_cpp(heatmap, paf, cfg)\n",
        "  return draw_humans(image, humans), humans\n",
        "\n",
        "rtpose_model = load_rtpose_model()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks/RT-multiperson-pose-pytorch\n",
            "Bulding VGG19\n",
            "Done !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byx2nJ0pYI5l"
      },
      "source": [
        "In order to display some results, images will be chosen at random to reproduce the paper's results. A grid will be created with some samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAFCo3hQXxKr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a1854dd-d0fa-4af9-964d-105915896712"
      },
      "source": [
        "def readFileList(file_directory, ext = 'jpg'):\n",
        "    files_list = []\n",
        "    if os.path.isdir(file_directory): # Return files with 'json' extension\n",
        "        for root_path, _, files_name in os.walk(file_directory):\n",
        "            files_list += [os.path.join(root_path, element) for element in files_name if element.split(\".\")[-1].lower() == ext.lower()]\n",
        "    elif file_directory.split('.')[-1] == ext: files_list = [file_directory] # Return file inside of list\n",
        "    return files_list\n",
        "\n",
        "file_list = readFileList(\"./data/coco/images/test2017\")\n",
        "if len(file_list) > 0:\n",
        "    print(\"[INFO]: Images through RT-multiperson pose 2D:\")\n",
        "    fig = plt.figure(figsize=(20, 15), constrained_layout=False)\n",
        "    gs = fig.add_gridspec(nrows=30, ncols=19, wspace=0.0, hspace=0.0)\n",
        "    fig_axes = {\n",
        "        fig.add_subplot(gs[:12, :6]): {\"id\": \"image1_1\"}, # First column\n",
        "        fig.add_subplot(gs[12:20, :6]): {\"id\": \"image2_1\"},\n",
        "        fig.add_subplot(gs[20:, :7]): {\"id\": \"image3_1\"},\n",
        "        fig.add_subplot(gs[:8, 6:13]): {\"id\": \"image1_2\"}, # Second column\n",
        "        fig.add_subplot(gs[8:20, 6:13]): {\"id\": \"image2_2\"},\n",
        "        fig.add_subplot(gs[20:, 7:13]): {\"id\": \"image3_2\"},\n",
        "        fig.add_subplot(gs[:7, 13:]): {\"id\": \"image7\"}, # Third column\n",
        "        fig.add_subplot(gs[7:16, 13:]): {\"id\": \"image8\"},\n",
        "        fig.add_subplot(gs[16:22, 13:]): {\"id\": \"image9\"},\n",
        "        fig.add_subplot(gs[22:, 13:]): {\"id\": \"image10\"},\n",
        "    }\n",
        "\n",
        "    for ax, prop in fig_axes.items():\n",
        "        human_det = []\n",
        "        while len(human_det) == 0:\n",
        "            image = cv2.imread(np.random.choice(file_list))\n",
        "            image_rt, human_det = human_forward(image, rtpose_model)\n",
        "        ax.imshow(cv2.cvtColor(image_rt, cv2.COLOR_BGR2RGB), aspect = \"auto\")\n",
        "        ax.set_xticklabels([]); ax.set_yticklabels([])\n",
        "        ax.set_xticks([]); ax.set_yticks([]); ax.axis(\"on\")\n",
        "    fig.show()\n",
        "else:\n",
        "    print(\"[INFO]: Not image found. Please check image folder\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO]: Not image found. Please check image folder\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0Ejah7GdXBN"
      },
      "source": [
        "isrun = False\n",
        "try:\n",
        "    if isrun:\n",
        "        run_eval(image_dir= './data/coco/images/val2017', \n",
        "          anno_file = './data/coco/annotations/annotations/person_keypoints_val2017.json', \n",
        "          vis_dir = './data/coco/images/vis_val2017', model=model, preprocess='vgg')\n",
        "except:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgimQ8_3u04V"
      },
      "source": [
        "The summary of the evaluation statistics are presented below. For the validation set, we can see that the model is recognizing about 13% of the cases correctly, of which we can be sure that about 20% are being well detected.\n",
        "\n",
        "<table>\n",
        "<tbody>\n",
        "  <tr>\n",
        "    <th>Average Precision (AP)</th> <th>IoU=0.50:0.95</th> <th>area = all</th> <th>maxDets = 20</th> <th>0.091</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <th>Average Precision (AP)</th> <th>IoU=0.50</th> <th>area = all</th> <th>maxDets = 20</th> <th>0.223</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <th>Average Precision (AP)</th> <th>IoU=0.75</th> <th>area = all</th> <th>maxDets = 20</th> <th>0.057</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <th>Average Precision (AP)</th> <th>IoU=0.50:0.95</th> <th>area = medium</th> <th>maxDets = 20</th> <th>0.131</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <th>Average Precision (AP)</th> <th>IoU=0.50:0.95</th> <th>area = large</th> <th>maxDets = 20</th> <th>0.091</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <th>Average Recall (AR)</th> <th>IoU=0.50:0.95</th> <th>area = all</th> <th>maxDets = 20</th> <th>0.188</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <th>Average Recall (AR)</th> <th>IoU=0.50</th> <th>area = all</th> <th>maxDets = 20</th> <th>0.350</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <th>Average Recall (AR)</th> <th>IoU=0.75</th> <th>area = all</th> <th>maxDets = 20</th> <th>0.167</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <th>Average Recall (AR)</th> <th>IoU=0.50:0.95</th> <th>area = medium</th> <th>maxDets = 20</th> <th>0.140</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <th>Average Recall (AR)</th> <th>IoU=0.50:0.95</th> <th>area = large</th> <th>maxDets = 20</th> <th>0.255</th>\n",
        "  </tr>\n",
        "</tbody>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAApuRJK7JxM"
      },
      "source": [
        "## Section 3: Applications\n",
        "Some applications in the field of deep learning have been developed in recent years with themes relating to the detection of the position of people in different scenes. Some of these include the classification of postures, detection of people in position, robots assisted living, character animation, video games industry, medical applications such as postural corrections, and anothers [interesting projects](https://medium.com/beyondminds/an-overview-of-human-pose-estimation-with-deep-learning-d49eb656739b). Below are some articles of interest:\n",
        "\n",
        "1.   [Multi-Person Pose Estimation for Pose Tracking with Enhanced Cascaded Pyramid Network](https://openaccess.thecvf.com/content_ECCVW_2018/papers/11130/Yu_Multi-Person_Pose_Estimation_for_Pose_Tracking_with_Enhanced_Cascaded_Pyramid_ECCVW_2018_paper.pdf)\n",
        "2.   [Single-Stage Multi-Person Pose Machines](https://arxiv.org/pdf/1908.09220.pdf)\n",
        "3.   [Rehabilitation Posture Correction Using Deep Neural\n",
        "Network](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7881743)\n",
        "4.   [Pose Trainer: Correcting Exercise Posture using Pose Estimation](https://arxiv.org/abs/2006.11718)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ysg5djnUntE"
      },
      "source": [
        "In this notebook, we will attack in a particular aplication: The detection of multiple person poses in videos and their prediction in later frames. For this, we will use the [`tv_human_interactions`](https://www.robots.ox.ac.uk/~vgg/data/tv_human_interactions/) database."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Wwp9Gc9Bg2Q"
      },
      "source": [
        "### Problem context\n",
        "\n",
        "We must understand that for every image in the video, we have that the network predicts an affinity map and a body part map, with sizes  $(46,83,38)$ and $(46,83,19)$, respectively. To predict the pose in future frames of a video we will use this information as an RCNN target, coupling each detection in a single tensor. The idea will be to be able to predict these two maps of a given frame a map of the current state, coupled into a single map set of $(46,83,57)$ size for a single image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYn0nymvc1_E"
      },
      "source": [
        "### Download database\n",
        "\n",
        "As mentioned above, the database will be downloaded from [`tv_human_interactions`](https://www.robots.ox.ac.uk/~vgg/data/tv_human_interactions/). A state flag will allow you to switch between downloading and not downloading the database."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hR5UNP5Xdc2w"
      },
      "source": [
        "download_tv_human = False\n",
        "if download_tv_human and not os.path.isdir(\"./data/tv_human_interactions_videos\"):\n",
        "    if not os.path.isdir(\"./data\"): os.mkdir(\"./data\")\n",
        "    !wget -P \"./data\" \"https://www.robots.ox.ac.uk/~vgg/data/tv_human_interactions/data/tv_human_interactions_videos.tar.gz\"\n",
        "    !tar -xzvf \"./data/tv_human_interactions_videos.tar.gz\" -C \"./data\"\n",
        "    os.remove(\"./data/tv_human_interactions_videos.tar.gz\")\n",
        "    output.clear()\n",
        "    print(\"[INFO] TV Human Interactions database download succesfully!.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hpcf2siNR6Qi"
      },
      "source": [
        "### Architecture implementation in videos\n",
        "Let's start with the implementation of the architecture in a test video. We will make the frame to frame reading, showing the result of the estimation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9BuVeQxSEab"
      },
      "source": [
        "''' Extract properties from video '''\n",
        "def video_prop_read(video_path, force_mp4 = True):\n",
        "    video = cv2.VideoCapture(video_path) # Read video\n",
        "    w,h = int(video.get(cv2.CAP_PROP_FRAME_WIDTH)), int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    video_fps = video.get(cv2.CAP_PROP_FPS)\n",
        "    codec = [chr((int(video.get(cv2.CAP_PROP_FOURCC)) >> 8 * i) & 0xFF) for i in range(4)]\n",
        "    video_size = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    video.release()\n",
        "    if \"mp4\" in os.path.splitext(video_path)[-1].lower() or force_mp4: codec = ['m', 'p', '4', 'v']\n",
        "    if video_size <= 0: video_size = np.inf\n",
        "    if video_fps <= 0 or video_fps == np.inf: video_fps = 25 # Default value\n",
        "    return w, h, video_fps, codec, video_size\n",
        "\n",
        "''' Show embed video'''\n",
        "def show_video(video_path):\n",
        "    filename, ext = os.path.splitext(video_path)\n",
        "    if os.path.isfile(video_path):\n",
        "        if \"mp4\" not in ext.lower():\n",
        "            filename += \".mp4\"\n",
        "            !sudo ffmpeg -t 5 -i \"$video_path\" \"$filename\" # Convert any ext to mp4\n",
        "            output.clear()\n",
        "            video_path = filename\n",
        "        video = Video.from_file(video_path)\n",
        "    else:\n",
        "        print(\"[ERROR] Video file not found. Please check path.\")\n",
        "        video = None\n",
        "    return video\n",
        "\n",
        "''' Draw humans in video '''\n",
        "def video_forward(video_in, fps = None, video_out_path = None, force_mp4 = True):\n",
        "    # Read video properties\n",
        "    video_w, video_h, video_fps, video_codec, video_size = video_prop_read(video_in, force_mp4)\n",
        "    video = cv2.VideoCapture(video_in)\n",
        "\n",
        "    # Video object to save and video_in read\n",
        "    if fps is not None and fps < video_fps: video_fps = fps\n",
        "    if video_out_path is not None:\n",
        "        video_out_path, ext = os.path.splitext(video_out_path)\n",
        "        if force_mp4: video_out_path += \".mp4\"\n",
        "        elif len(ext) == 0: video_out_path += os.path.splitext(video_in)[-1]\n",
        "        else: video_out_path += ext\n",
        "        \n",
        "        video_out = cv2.VideoWriter(video_out_path, cv2.VideoWriter_fourcc(*video_codec), \n",
        "                                    video_fps, (video_w, video_h))\n",
        "        print(\"[INFO]: Video will be saved in\", video_out_path)\n",
        "\n",
        "    fcount = 0; tic = time.time(); predictions = None\n",
        "    while video.isOpened():\n",
        "        ret, frame = video.read(); fcount += 1\n",
        "        if fps is not None and fcount % (fps//video_fps) != 0: continue # Skip frames\n",
        "        if (cv2.waitKey(1) & 0xFF == ord('q')) or not ret: break # End of video\n",
        "\n",
        "        # Detection process\n",
        "        paf, heatmap, im_scale = im_forward(frame, rtpose_model) # CNN maps\n",
        "        humans = paf_to_pose_cpp(heatmap, paf, cfg)\n",
        "        if len(humans) > 0: # Save only frames with predictions\n",
        "            predic = np.append(paf, heatmap, axis = -1)[None]\n",
        "            if predictions is None:\n",
        "                predictions = predic # Expand dimension\n",
        "            else:\n",
        "                predictions = np.append(predictions, predic, axis = 0)\n",
        "\n",
        "        # Save pose-detection in video_out\n",
        "        if video_out_path is not None:\n",
        "            frame_out = draw_humans(frame, humans)\n",
        "            video_out.write(frame)\n",
        "        \n",
        "        # if isprogrammer: cv2.imshow(\"output.mp4\", frame)\n",
        "        if fcount % 30 == 0:\n",
        "            print(\"[INFO]: {} of {} frames processed.\".format(fcount, video_size))\n",
        "        \n",
        "    video.release()\n",
        "    if video_out_path is not None: \n",
        "        video_out.release()\n",
        "        if \"mp4\" in os.path.splitext(video_out_path)[-1]:\n",
        "            video_out_path_compress = video_out_path.replace(\".mp4\",\"_out.mp4\")\n",
        "            !sudo ffmpeg -t 5 -i \"$video_out_path\" \"$video_out_path_compress\" # Compress\n",
        "            !mv \"$video_out_path_compress\" \"$video_out_path\"\n",
        "            !rm \"$video_out_path_compress\"\n",
        "            output.clear()\n",
        "        print(\"[INFO]: Video saved successfully\")\n",
        "    print(\"[INFO]: Total time spend in procedure:\", time.time() - tic, \"s\")\n",
        "    return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwoU7KeEaYr5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84af69c7-8bfb-41ff-edd0-a8ac4eaf3613"
      },
      "source": [
        "video_path_proof = \"./data/tv_human_interactions_videos/handShake_0009.avi\"\n",
        "demo_pred = video_forward(video_path_proof, video_out_path = \"./demo/video_demo\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO]: Video saved successfully\n",
            "[INFO]: Total time spend in procedure: 32.04157209396362 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Rg8J-ACbpon",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409,
          "referenced_widgets": [
            "05eadac4b19e4315a7ca29097abd76d4",
            "f86a523a575b4ec6b86cd6097c5a48ee"
          ]
        },
        "outputId": "ff51800d-145d-4218-c180-2f9114e9cd37"
      },
      "source": [
        "show_video(\"./demo/video_demo.mp4\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "05eadac4b19e4315a7ca29097abd76d4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Video(value=b'\\x00\\x00\\x00 ftypisom\\x00\\x00\\x02\\x00isomiso2avc1mp41\\x00\\x00\\x00\\x08free\\x00\\x08K#mdat\\x00\\x00\\…"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8_20lTfccOm"
      },
      "source": [
        "### Pose-database creation\n",
        "After we have tested the network in a simple video, we will split the video list files into test, train and validation sets.\n",
        "\n",
        "In addition, we will perform a mini-batch training, where each batche will be represented by each video. Therefore, we will define a function that will allow us to process each video and organize it in tensors, so that it organizes the information for the input/output training process. As the input is the composition of the prediction in $N$ previous states to predict the $N+1$ state (output), the dimenssions of the input tensor will be of $(x,N,46,83,57)$ and $(x,46,83,57)$ for the output, with $x$ the total of images in the video. If we desired $N=1$, $(x,46,83,57)$ will be the dimension of the entry tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDO1QRgScbkV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ebbf618-aeca-4265-d9c6-e3841db1635a"
      },
      "source": [
        "# Train, test and valid video lists creation\n",
        "random_state = 42\n",
        "video_list = readFileList(\"./data/tv_human_interactions_videos/\", ext = 'avi')\n",
        "videoloaders = {x:y for x,y in zip([\"train\",\"test\"], train_test_split(video_list, test_size = 0.3, random_state = random_state))}\n",
        "videoloaders.update({x:y for x,y in zip([\"valid\",\"test\"], train_test_split(videoloaders[\"test\"], test_size = 0.5, random_state = random_state))})\n",
        "video_set_sizes = {x: len(videoloaders[x]) for x in ['train', 'valid', 'test']}\n",
        "print(video_set_sizes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'train': 210, 'valid': 45, 'test': 45}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6t2KeflHEJ9I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ff0c315-742e-4223-ee7b-666bb978b64b"
      },
      "source": [
        "# Load (predict) video and split it in in/out tensor\n",
        "def video_batch(video_path, batch_size = 12): # (b,t,c,h,w)\n",
        "    predictions = video_forward(video_path) # Video forward in RT-model (x,h,w,c)\n",
        "    total_batch = None\n",
        "    for t in range(predictions.shape[0]//batch_size):\n",
        "        batch = predictions[batch_size*t:batch_size*(t+1)] # Batch of (batch_size,h,w,c) size\n",
        "        batch = np.transpose(batch[None], (1,0,4,2,3)) # Batch of (b,1,c,h,w) size\n",
        "        if total_batch is None:\n",
        "            total_batch = batch\n",
        "        else:\n",
        "            total_batch = np.append(total_batch, batch, axis = 1) # Append new batches\n",
        "    return torch.from_numpy(total_batch).float()\n",
        "    # OTHER FROM \n",
        "    if N == 1: # Only for one stage\n",
        "        input_tensor = torch.from_numpy(predictions[:-N]).float()\n",
        "    else: # N > 1\n",
        "        input_tensor = np.expand_dims(predictions[:-N], axis = 1) # First state\n",
        "        for n in range(1,N):\n",
        "            i_tensor = np.expand_dims(predictions[n:-N+n], axis = 1)\n",
        "            input_tensor = np.append(input_tensor, i_tensor, axis = 1)\n",
        "        input_tensor = torch.from_numpy(input_tensor).float()\n",
        "    output_tensor = torch.from_numpy(predictions[N:]).float()\n",
        "    return input_tensor, output_tensor\n",
        "\n",
        "batch_proof = video_batch(videoloaders[\"train\"][10])\n",
        "print(batch_proof.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO]: 30 of 110 frames processed.\n",
            "[INFO]: 60 of 110 frames processed.\n",
            "[INFO]: 90 of 110 frames processed.\n",
            "[INFO]: Total time spend in procedure: 24.082513570785522 s\n",
            "torch.Size([12, 9, 57, 46, 82])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IFqXrABMBmT"
      },
      "source": [
        "### RCNN architecture definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LI692SOeSJfN"
      },
      "source": [
        "''' Defined a single convLSTM module '''\n",
        "\n",
        "class ConvLSTMCell(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\n",
        "        \"\"\"\n",
        "        Initialize ConvLSTM cell.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        input_dim: int\n",
        "            Number of channels of input tensor.\n",
        "        hidden_dim: int\n",
        "            Number of channels of hidden state.\n",
        "        kernel_size: (int, int)\n",
        "            Size of the convolutional kernel.\n",
        "        bias: bool\n",
        "            Whether or not to add the bias.\n",
        "        \"\"\"\n",
        "\n",
        "        super(ConvLSTMCell, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.kernel_size = kernel_size\n",
        "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
        "        self.bias = bias\n",
        "\n",
        "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
        "                              out_channels=4 * self.hidden_dim,\n",
        "                              kernel_size=self.kernel_size,\n",
        "                              padding=self.padding, bias=self.bias)\n",
        "\n",
        "    def forward(self, input_tensor, cur_state):\n",
        "        h_cur, c_cur = cur_state\n",
        "\n",
        "        combined = torch.cat([input_tensor, h_cur], dim=1)  # concatenate along channel axis\n",
        "\n",
        "        combined_conv = self.conv(combined)\n",
        "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
        "        i = torch.sigmoid(cc_i)\n",
        "        f = torch.sigmoid(cc_f)\n",
        "        o = torch.sigmoid(cc_o)\n",
        "        g = torch.tanh(cc_g)\n",
        "\n",
        "        c_next = f * c_cur + i * g\n",
        "        h_next = o * torch.tanh(c_next)\n",
        "\n",
        "        return h_next, c_next\n",
        "\n",
        "    def init_hidden(self, batch_size, image_size):\n",
        "        height, width = image_size\n",
        "        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device),\n",
        "                torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkMZRCoESYfm"
      },
      "source": [
        "class EncoderDecoderConvLSTM(nn.Module):\n",
        "    def __init__(self, nf, in_chan, future_step):\n",
        "        super(EncoderDecoderConvLSTM, self).__init__()\n",
        "\n",
        "        \"\"\" ARCHITECTURE \n",
        "\n",
        "        # Encoder (ConvLSTM)\n",
        "        # Encoder Vector (final hidden state of encoder)\n",
        "        # Decoder (ConvLSTM) - takes Encoder Vector as input\n",
        "        # Decoder (3D CNN) - produces regression predictions for our model\n",
        "\n",
        "        \"\"\"\n",
        "        self.encoder_1_convlstm = ConvLSTMCell(input_dim=in_chan, hidden_dim=nf, kernel_size=(3, 3), bias=True)\n",
        "        self.encoder_2_convlstm = ConvLSTMCell(input_dim=nf, hidden_dim=nf, kernel_size=(3, 3), bias=True)\n",
        "        self.decoder_1_convlstm = ConvLSTMCell(input_dim=nf, hidden_dim=nf, kernel_size=(3, 3), bias=True)\n",
        "        self.decoder_2_convlstm = ConvLSTMCell(input_dim=nf, hidden_dim=nf, kernel_size=(3, 3), bias=True)\n",
        "        self.decoder_CNN = nn.Conv3d(in_channels=nf, out_channels=1, kernel_size=(1, 3, 3), padding=(0, 1, 1))\n",
        "        self.future_step = future_step\n",
        "\n",
        "    def autoencoder(self, x, seq_len, h_t, c_t, h_t2, c_t2, h_t3, c_t3, h_t4, c_t4):\n",
        "        outputs = []\n",
        "        # encoder\n",
        "        for t in range(seq_len):\n",
        "            h_t, c_t = self.encoder_1_convlstm(input_tensor=x[:, t, :, :, :],\n",
        "                                               cur_state=[h_t, c_t])  # we could concat to provide skip conn here\n",
        "            h_t2, c_t2 = self.encoder_2_convlstm(input_tensor=h_t,\n",
        "                                                 cur_state=[h_t2, c_t2])  # we could concat to provide skip conn here\n",
        "        # encoder_vector\n",
        "        encoder_vector = h_t2\n",
        "\n",
        "        # decoder\n",
        "        for t in range(self.future_step):\n",
        "            h_t3, c_t3 = self.decoder_1_convlstm(input_tensor=encoder_vector,\n",
        "                                                 cur_state=[h_t3, c_t3])  # we could concat to provide skip conn here\n",
        "            h_t4, c_t4 = self.decoder_2_convlstm(input_tensor=h_t3,\n",
        "                                                 cur_state=[h_t4, c_t4])  # we could concat to provide skip conn here\n",
        "            encoder_vector = h_t4\n",
        "            outputs += [h_t4]  # predictions\n",
        "\n",
        "        outputs = torch.stack(outputs, 1)\n",
        "        outputs = outputs.permute(0, 2, 1, 3, 4)\n",
        "        outputs = self.decoder_CNN(outputs)\n",
        "        outputs = torch.nn.Sigmoid()(outputs)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def forward(self, x, hidden_state=None):\n",
        "\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        input_tensor:\n",
        "            5-D Tensor of shape (b, t, c, h, w) # batch, time, channel, height, width\n",
        "        \"\"\"\n",
        "\n",
        "        # find size of different input dimensions\n",
        "        b, seq_len, _, h, w = x.size()\n",
        "\n",
        "        # initialize hidden states\n",
        "        h_t, c_t = self.encoder_1_convlstm.init_hidden(batch_size=b, image_size=(h, w))\n",
        "        h_t2, c_t2 = self.encoder_2_convlstm.init_hidden(batch_size=b, image_size=(h, w))\n",
        "        h_t3, c_t3 = self.decoder_1_convlstm.init_hidden(batch_size=b, image_size=(h, w))\n",
        "        h_t4, c_t4 = self.decoder_2_convlstm.init_hidden(batch_size=b, image_size=(h, w))\n",
        "\n",
        "        # autoencoder forward\n",
        "        outputs = self.autoencoder(x, seq_len, h_t, c_t, h_t2, c_t2, h_t3, c_t3, h_t4, c_t4)\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEpx0dAMTUfh"
      },
      "source": [
        "### Training model definition\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4nkUuEkTTvG"
      },
      "source": [
        "def training(EDconvLST_model, criterion, optimizer, scheduler = None, \n",
        "             num_epochs = 25, model_name = None, early_max = None, \n",
        "             batch_size = 12, n_steps_past = 10):\n",
        "    since, count_max = time.time(), 0\n",
        "    \n",
        "    tic = time.time(); losses = {\"train\": [], \"valid\": []}\n",
        "    stats = {\"losses\": {\"train\": [], \"valid\": []}, # Variable to save all\n",
        "            \"best_epoch\": 0, \"best_loss\": np.inf, \"time_process\": 0} \n",
        "    for epoch in range(1, num_epochs+1):\n",
        "        isprint = num_epochs <= 100 or (epoch-1) % (num_epochs // 10) == 0 or epoch == num_epochs\n",
        "        if isprint: print('\\nEpoch {}/{}\\n{}'.format(epoch, num_epochs, '-'*15))\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'valid']:\n",
        "            if phase == 'train': EDconvLST_model.train()  # Set model to training mode\n",
        "            else: EDconvLST_model.eval() # Set model to evaluate mode\n",
        "\n",
        "            # Iterate over data.\n",
        "            running_loss = 0.0\n",
        "            for video_path in videoloaders[phase][:2]: # Start for\n",
        "                batch = video_batch(video_path, batch_size = batch_size) # Batch of (b,t,c,h,w)\n",
        "                x, y = batch[:, 0:n_steps_past, :, :, :], batch[:, n_steps_past:, :, :, :]\n",
        "                x, y = x.cuda(), y.cuda()\n",
        "\n",
        "                # forward\n",
        "                optimizer.zero_grad() # zero the parameter gradients\n",
        "                with torch.set_grad_enabled(phase == 'train'): # track history if only in train\n",
        "                    y_hat = EDconvLST_model(x)\n",
        "                    loss = criterion(y_hat, y)\n",
        "                    if phase == 'train': # backward + optimize only if in training phase\n",
        "                        loss.backward(); optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * x.size(0)\n",
        "            \n",
        "            if phase == 'train' and scheduler is not None: scheduler.step()\n",
        "            stats[\"losses\"][phase].append(running_loss/video_set_sizes[phase])\n",
        "        \n",
        "        time_elapsed = time.time() - tic\n",
        "        if isprint: # Print somethings for some epoch\n",
        "            print('train Loss: {:.6f} \\tvalid Loss: {:.6f}. Spend time:{:.0f}m {:.0f}s'\\\n",
        "                .format(stats[\"losses\"][\"train\"][-1], stats[\"losses\"][\"valid\"][-1], time_elapsed // 60, time_elapsed % 60))\n",
        "            tic = time.time() # For next iteration\n",
        "#         print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "        # deep save model\n",
        "        if stats[\"losses\"][\"valid\"][-1] < stats[\"best_loss\"]:\n",
        "            if isprint: \n",
        "                print('Valid loss decreased ({:.6f} --> {:.6f}). Saving model ...'.format(stats[\"best_loss\"], stats[\"losses\"][\"valid\"][-1]))\n",
        "            stats[\"best_epoch\"] = epoch - 1\n",
        "            stats[\"best_loss\"] = stats[\"losses\"][\"valid\"][-1]\n",
        "            best_model_wts = copy.deepcopy(EDconvLST_model.state_dict())\n",
        "            if model_name is not None: torch.save(best_model_wts, model_name)\n",
        "            count_max = 0\n",
        "        else: \n",
        "            count_max += 1 \n",
        "            if early_max is not None and count_max >= early_max: break # Early finish training\n",
        "\n",
        "    stats[\"time_process\"] = time.time() - since\n",
        "    print('\\n{}\\nTraining complete ({}/{} epochs) in {:.0f}m {:.0f}s'\\\n",
        "          .format('-'*25, epoch, num_epochs, stats[\"time_process\"] // 60, stats[\"time_process\"] % 60))\n",
        "    print('Best metric (loss): {:4f}'.format(stats[\"best_loss\"]))\n",
        "\n",
        "    # load best model weights\n",
        "    EDconvLST_model.load_state_dict(best_model_wts)\n",
        "    EDconvLST_model.cuda()\n",
        "    return EDconvLST_model, stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28xlkBcmbEWQ"
      },
      "source": [
        "### User parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eG-HettwbD1S"
      },
      "source": [
        "EDCLSTM_model = EncoderDecoderConvLSTM(nf = 57, in_chan = 57, future_step = 57)\n",
        "EDCLSTM_model.cuda()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(EDCLSTM_model.parameters(), lr = 1e-4, betas=(0.9, 0.98))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZfaZkBhcl08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f403e3b9-3890-4182-d5da-061aeb79add4"
      },
      "source": [
        "EDCLSTM_model, EDCLSTM_stats = training(EDCLSTM_model, criterion, optimizer, scheduler = None, \n",
        "             num_epochs = 4, model_name = None, early_max = None, \n",
        "             batch_size = 5, n_steps_past = 2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/4\n",
            "---------------\n",
            "[INFO]: 30 of 75 frames processed.\n",
            "[INFO]: 60 of 75 frames processed.\n",
            "[INFO]: Total time spend in procedure: 16.268709659576416 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:446: UserWarning: Using a target size (torch.Size([5, 13, 57, 46, 84])) that is different to the input size (torch.Size([5, 1, 57, 46, 84])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO]: 30 of 44 frames processed.\n",
            "[INFO]: Total time spend in procedure: 7.671265363693237 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:446: UserWarning: Using a target size (torch.Size([5, 7, 57, 46, 62])) that is different to the input size (torch.Size([5, 1, 57, 46, 62])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01px-RNbmc5T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "outputId": "9bc09199-cd49-408e-e06b-200e17f9e07c"
      },
      "source": [
        "def loss_plot(losses):\n",
        "    for phase in [\"train\", \"valid\"]:\n",
        "        plt.plot(range(1, len(losses[phase]) + 1), losses[phase], label = phase + \" losses\")\n",
        "    plt.legend(prop = {'size': 10})\n",
        "    plt.title('loss function', size = 10)\n",
        "    plt.xlabel('epoch', size = 10); plt.ylabel('loss value', size = 10)\n",
        "\n",
        "loss_plot(EDCLSTM_stats[\"losses\"])\n",
        "EDCLSTM_stats"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'best_epoch': 0,\n",
              " 'best_loss': 0.057355652252833046,\n",
              " 'losses': {'train': [0.012350496791657947], 'valid': [0.057355652252833046]},\n",
              " 'time_process': 50.14371633529663}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuGuIP6j7dc9"
      },
      "source": [
        "## ***Anexos: GitHub connection***\n",
        "Here, some functions to upload the github respository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XM88fGToxGPB"
      },
      "source": [
        "''' Function definitions'''\n",
        "# Git pull\n",
        "def git_pull(repo_pwd, show_current_branch = False, make_commit = False): # Only for colab space work\n",
        "    global user_git, email_git\n",
        "    import sys\n",
        "    IN_COLAB = 'google.colab' in sys.modules\n",
        "    if IN_COLAB:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/gdrive')\n",
        "\n",
        "        %cd \"$repo_pwd\"\n",
        "        # !git config --list\n",
        "        if show_current_branch: \n",
        "            !git branch \n",
        "        if make_commit:\n",
        "            if \"user_git\" not in globals(): user_git = input(\"User github?: \")\n",
        "            if \"email_git\" not in globals(): email_git = input(\"Email github?: \") \n",
        "            !git config --global user.email \"$email_git\"\n",
        "            !git config --global user.name \"$user_git\"\n",
        "            !git commit -am \"Updating in colab\"\n",
        "        !git pull\n",
        "        !git status\n",
        "    else:\n",
        "        print(\"[INFO] You are not in collaboration, nothing has been done.\")\n",
        "\n",
        "# Git push\n",
        "def git_push(repo_pwd): # Only for colab space work\n",
        "    global user_git, email_git\n",
        "    import sys\n",
        "    IN_COLAB = 'google.colab' in sys.modules\n",
        "    if IN_COLAB:\n",
        "        from google.colab import drive\n",
        "        import getpass\n",
        "        drive.mount('/content/gdrive')\n",
        "\n",
        "        %cd \"$repo_pwd\"\n",
        "        if \"user_git\" not in globals(): user_git = input(\"User github?: \")\n",
        "        if \"email_git\" not in globals(): email_git = input(\"Email github?: \")\n",
        "\n",
        "        # Password login\n",
        "        try: \n",
        "            pwd_git = getpass.getpass(prompt='{} github password: '.format(user_git)) \n",
        "        except Exception as error: \n",
        "            print('ERROR', error) \n",
        "\n",
        "        # Upload from every where\n",
        "        origin_git = !git config --get remote.origin.url\n",
        "        origin_git = origin_git[0].replace(\"https://\",\"https://{}:{}@\".format(user_git,pwd_git))\n",
        "\n",
        "        !git config --global user.email \"$email_git\"\n",
        "        !git config --global user.name \"$user_git\"\n",
        "        !git status\n",
        "\n",
        "        x = \" \"\n",
        "        while x.lower() != \"y\" and x.lower() != \"n\": x = input(\"Continue?...[y/n]: \")\n",
        "\n",
        "        if x.lower() == \"y\":\n",
        "            com_message = input(\"Enter the commit message: \")\n",
        "            !git add .\n",
        "            !git commit -am \"$com_message\"\n",
        "            !git push \"$origin\"\n",
        "            !git status\n",
        "    else:\n",
        "        print(\"[INFO] You are not in collaboration, nothing has been done.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOTNnyQsDfXb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51570747-c337-4327-b246-d9eaf05e0628"
      },
      "source": [
        "repo_pwd = \"/content/gdrive/My Drive/Colab Notebooks/RT-multiperson-pose-pytorch\"\n",
        "# git_pull(repo_pwd, show_current_branch = False, make_commit = True)\n",
        "# git_push(repo_pwd)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "/content/gdrive/My Drive/Colab Notebooks/RT-multiperson-pose-pytorch\n",
            "Johansmm github password: ··········\n",
            "On branch imt-atlantique\n",
            "Your branch is ahead of 'origin/imt-atlantique' by 3 commits.\n",
            "  (use \"git push\" to publish your local commits)\n",
            "\n",
            "nothing to commit, working tree clean\n",
            "Continue?...[y/n]: y\n",
            "Enter the commit message: update repo\n",
            "On branch imt-atlantique\n",
            "Your branch is ahead of 'origin/imt-atlantique' by 3 commits.\n",
            "  (use \"git push\" to publish your local commits)\n",
            "\n",
            "nothing to commit, working tree clean\n",
            "fatal: No path specified. See 'man git-pull' for valid url syntax\n",
            "On branch imt-atlantique\n",
            "Your branch is ahead of 'origin/imt-atlantique' by 3 commits.\n",
            "  (use \"git push\" to publish your local commits)\n",
            "\n",
            "nothing to commit, working tree clean\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GY2pPGvf8bad"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}