{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Deep-Learning-project\" data-toc-modified-id=\"Deep-Learning-project-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Deep Learning project</a></span><ul class=\"toc-item\"><li><span><a href=\"#Section-0:-Download-and-install-repository\" data-toc-modified-id=\"Section-0:-Download-and-install-repository-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Section 0: Download and install repository</a></span></li><li><span><a href=\"#Section-2:-Replication-of-results\" data-toc-modified-id=\"Section-2:-Replication-of-results-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Section 2: Replication of results</a></span></li><li><span><a href=\"#Section-3:-Applications\" data-toc-modified-id=\"Section-3:-Applications-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Section 3: Applications</a></span><ul class=\"toc-item\"><li><span><a href=\"#Problem-context\" data-toc-modified-id=\"Problem-context-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Problem context</a></span></li><li><span><a href=\"#Download-database\" data-toc-modified-id=\"Download-database-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Download database</a></span></li><li><span><a href=\"#Architecture-implementation-in-videos\" data-toc-modified-id=\"Architecture-implementation-in-videos-1.3.3\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;</span>Architecture implementation in videos</a></span></li><li><span><a href=\"#Pose-database-creation\" data-toc-modified-id=\"Pose-database-creation-1.3.4\"><span class=\"toc-item-num\">1.3.4&nbsp;&nbsp;</span>Pose-database creation</a></span></li><li><span><a href=\"#RCNN-architecture-definition\" data-toc-modified-id=\"RCNN-architecture-definition-1.3.5\"><span class=\"toc-item-num\">1.3.5&nbsp;&nbsp;</span>RCNN architecture definition</a></span></li><li><span><a href=\"#Training-model-definition\" data-toc-modified-id=\"Training-model-definition-1.3.6\"><span class=\"toc-item-num\">1.3.6&nbsp;&nbsp;</span>Training model definition</a></span></li><li><span><a href=\"#User-parameters\" data-toc-modified-id=\"User-parameters-1.3.7\"><span class=\"toc-item-num\">1.3.7&nbsp;&nbsp;</span>User parameters</a></span></li></ul></li><li><span><a href=\"#Anexos:-GitHub-connection\" data-toc-modified-id=\"Anexos:-GitHub-connection-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span><strong><em>Anexos: GitHub connection</em></strong></a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xrjZTtw2rMCf"
   },
   "source": [
    "# Deep Learning project\n",
    "Intro by Tatiana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QiaLI5xU4EeM",
    "outputId": "bf1c215b-1024-4c6f-fa94-36a52aff02e9"
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "# Colab libraries\n",
    "from google.colab import drive\n",
    "from google.colab import output\n",
    "drive.mount('/content/gdrive')\n",
    "colab_path = \"/content/gdrive/My Drive/Colab Notebooks/\"\n",
    "\n",
    "# Basis libraries\n",
    "import os, re, sys, math, time, scipy, argparse, copy\n",
    "import cv2, matplotlib, json\n",
    "import numpy as np\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import Video\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torchsummary import summary\n",
    "from collections import OrderedDict\n",
    "from scipy.ndimage.morphology import generate_binary_structure\n",
    "from scipy.ndimage.filters import gaussian_filter, maximum_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1b25TcW4ik9"
   },
   "source": [
    "## Section 0: Download and install repository\n",
    "First, we will download the repository that we copied from the [original repository](https://github.com/tensorboy/pytorch_Realtime_Multi-Person_Pose_Estimation) in order to make some changes for educational purposes. Then, we install libraries and some dependences explained in the original repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dKtbnvkHrBfA",
    "outputId": "db48703c-b67f-4775-9b7f-2d21bbdd2fac"
   },
   "outputs": [],
   "source": [
    "# Independence install\n",
    "!sudo apt-get install swig\n",
    "%cd $colab_path\n",
    "if not os.path.isdir(\"RT-multiperson-pose-pytorch\"):\n",
    "  # Repository clone\n",
    "  !git clone https://github.com/Johansmm/RT-multiperson-pose-pytorch.git\n",
    "  %cd \"RT-multiperson-pose-pytorch\"\n",
    "  %cd lib/pafprocess \n",
    "  # Repository compile\n",
    "  !sh make.sh\n",
    "\n",
    "# Libraries install\n",
    "%cd $colab_path\"/RT-multiperson-pose-pytorch\"\n",
    "!python -m pip install -r ./requirements.txt\n",
    "!git submodule init && git submodule update\n",
    "# Weights download\n",
    "if not os.path.isfile(\"./pose_model.pth\"):\n",
    "  !wget https://www.dropbox.com/s/ae071mfm2qoyc8v/pose_model.pth\n",
    "output.clear()\n",
    "print(\"[INFO]: Proyect uploaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OXqO9ha9lQao",
    "outputId": "1bc0b3f8-ee8a-4eca-8710-041a1d11f1fe"
   },
   "outputs": [],
   "source": [
    "!python demo/picture_demo.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MNWcHBWR6K7n",
    "outputId": "505240df-5f88-44e6-f7ad-b75cc341ace0"
   },
   "outputs": [],
   "source": [
    "# Framework libraries\n",
    "%cd $colab_path\"/RT-multiperson-pose-pytorch\"\n",
    "sys.path.append('.')\n",
    "from lib.network.rtpose_vgg import get_model\n",
    "from lib.network import im_transform\n",
    "from lib.utils.common import Human, BodyPart, CocoPart, CocoColors, CocoPairsRender, draw_humans\n",
    "from lib.utils.paf_to_pose import paf_to_pose_cpp\n",
    "from lib.config import cfg, update_config\n",
    "from evaluate.coco_eval import get_outputs, handle_paf_and_heat, run_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "174xVRPR9QN-"
   },
   "source": [
    "## Section 2: Replication of results\n",
    "In this section we replicated some resutls. First, we need download the data. For this case, we will use the `sh` compiler provided by [original repository](https://github.com/tensorboy/pytorch_Realtime_Multi-Person_Pose_Estimation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3uBTojMrmtud",
    "outputId": "8bf0c6b3-1c15-479a-8d3d-d62551119824"
   },
   "outputs": [],
   "source": [
    "%cd $colab_path\"/RT-multiperson-pose-pytorch\"\n",
    "data_download = False # For download COCO dataset\n",
    "if data_download and not os.path.isdir(\"data/coco\"):\n",
    "  !mkdir data\n",
    "  %cd data\n",
    "  !sh ../lib/datasets/CocoDataDownloader.sh\n",
    "  %cd $colab_path\"/RT-multiperson-pose-pytorch\"\n",
    "  output.clear()\n",
    "  print(\"[INFO]: Coco database downloaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MekeFPVJ2gKJ"
   },
   "source": [
    "Now, we defined some principal functions and the neuronal network architecture.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "78deQRcV2mOz",
    "outputId": "b95294e1-cafd-4806-ee43-9d8c33c0cad4"
   },
   "outputs": [],
   "source": [
    "%cd $colab_path\"/RT-multiperson-pose-pytorch\"\n",
    "class Namespace:\n",
    "  def __init__(self, **kwargs):\n",
    "    self.__dict__.update(kwargs)\n",
    "\n",
    "def load_rtpose_model():\n",
    "  args = Namespace(cfg = './experiments/vgg19_368x368_sgd.yaml', weight = 'pose_model.pth', opts = [])\n",
    "  update_config(cfg, args)\n",
    "  model = get_model('vgg19')     \n",
    "  model.load_state_dict(torch.load(args.weight))\n",
    "  model = torch.nn.DataParallel(model).cuda()\n",
    "  model.float()\n",
    "  model.eval()\n",
    "  return model\n",
    "\n",
    "def im_forward(image, model):\n",
    "  with torch.no_grad():\n",
    "    paf, heatmap, im_scale = get_outputs(image, model, 'rtpose')\n",
    "  return paf, heatmap, im_scale\n",
    "\n",
    "def human_forward(image, model):\n",
    "  paf, heatmap, im_scale = im_forward(image, model)\n",
    "  humans = paf_to_pose_cpp(heatmap, paf, cfg)\n",
    "  return draw_humans(image, humans), humans\n",
    "\n",
    "rtpose_model = load_rtpose_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "byx2nJ0pYI5l"
   },
   "source": [
    "In order to display some results, images will be chosen at random to reproduce the paper's results. A grid will be created with some samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "id": "RAFCo3hQXxKr",
    "outputId": "e09f695b-115f-4e90-dd14-0fefab91ce51"
   },
   "outputs": [],
   "source": [
    "def readFileList(file_directory, ext = 'jpg'):\n",
    "    files_list = []\n",
    "    if os.path.isdir(file_directory): # Return files with 'json' extension\n",
    "        for root_path, _, files_name in os.walk(file_directory):\n",
    "            files_list += [os.path.join(root_path, element) for element in files_name if element.split(\".\")[-1].lower() == ext.lower()]\n",
    "    elif file_directory.split('.')[-1] == ext: files_list = [file_directory] # Return file inside of list\n",
    "    return files_list\n",
    "\n",
    "file_list = readFileList(\"./data/coco/images/test2017\")\n",
    "if len(file_list) > 0:\n",
    "    print(\"[INFO]: Images through RT-multiperson pose 2D:\")\n",
    "    fig = plt.figure(figsize=(20, 15), constrained_layout=False)\n",
    "    gs = fig.add_gridspec(nrows=30, ncols=19, wspace=0.0, hspace=0.0)\n",
    "    fig_axes = {\n",
    "        fig.add_subplot(gs[:12, :6]): {\"id\": \"image1_1\"}, # First column\n",
    "        fig.add_subplot(gs[12:20, :6]): {\"id\": \"image2_1\"},\n",
    "        fig.add_subplot(gs[20:, :7]): {\"id\": \"image3_1\"},\n",
    "        fig.add_subplot(gs[:8, 6:13]): {\"id\": \"image1_2\"}, # Second column\n",
    "        fig.add_subplot(gs[8:20, 6:13]): {\"id\": \"image2_2\"},\n",
    "        fig.add_subplot(gs[20:, 7:13]): {\"id\": \"image3_2\"},\n",
    "        fig.add_subplot(gs[:7, 13:]): {\"id\": \"image7\"}, # Third column\n",
    "        fig.add_subplot(gs[7:16, 13:]): {\"id\": \"image8\"},\n",
    "        fig.add_subplot(gs[16:22, 13:]): {\"id\": \"image9\"},\n",
    "        fig.add_subplot(gs[22:, 13:]): {\"id\": \"image10\"},\n",
    "    }\n",
    "\n",
    "    for ax, prop in fig_axes.items():\n",
    "        human_det = []\n",
    "        while len(human_det) == 0:\n",
    "            image = cv2.imread(np.random.choice(file_list))\n",
    "            image_rt, human_det = human_forward(image, rtpose_model)\n",
    "        ax.imshow(cv2.cvtColor(image_rt, cv2.COLOR_BGR2RGB), aspect = \"auto\")\n",
    "        ax.set_xticklabels([]); ax.set_yticklabels([])\n",
    "        ax.set_xticks([]); ax.set_yticks([]); ax.axis(\"on\")\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"[INFO]: Not image found. Please check image folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E0Ejah7GdXBN"
   },
   "outputs": [],
   "source": [
    "isrun = False\n",
    "try:\n",
    "    if isrun:\n",
    "        run_eval(image_dir= './data/coco/images/val2017', \n",
    "          anno_file = './data/coco/annotations/annotations/person_keypoints_val2017.json', \n",
    "          vis_dir = './data/coco/images/vis_val2017', model=model, preprocess='vgg')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FgimQ8_3u04V"
   },
   "source": [
    "The summary of the evaluation statistics are presented below. For the validation set, we can see that the model is recognizing about 13% of the cases correctly, of which we can be sure that about 20% are being well detected.\n",
    "\n",
    "<table>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <th>Average Precision (AP)</th> <th>IoU=0.50:0.95</th> <th>area = all</th> <th>maxDets = 20</th> <th>0.091</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Average Precision (AP)</th> <th>IoU=0.50</th> <th>area = all</th> <th>maxDets = 20</th> <th>0.223</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Average Precision (AP)</th> <th>IoU=0.75</th> <th>area = all</th> <th>maxDets = 20</th> <th>0.057</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Average Precision (AP)</th> <th>IoU=0.50:0.95</th> <th>area = medium</th> <th>maxDets = 20</th> <th>0.131</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Average Precision (AP)</th> <th>IoU=0.50:0.95</th> <th>area = large</th> <th>maxDets = 20</th> <th>0.091</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Average Recall (AR)</th> <th>IoU=0.50:0.95</th> <th>area = all</th> <th>maxDets = 20</th> <th>0.188</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Average Recall (AR)</th> <th>IoU=0.50</th> <th>area = all</th> <th>maxDets = 20</th> <th>0.350</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Average Recall (AR)</th> <th>IoU=0.75</th> <th>area = all</th> <th>maxDets = 20</th> <th>0.167</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Average Recall (AR)</th> <th>IoU=0.50:0.95</th> <th>area = medium</th> <th>maxDets = 20</th> <th>0.140</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Average Recall (AR)</th> <th>IoU=0.50:0.95</th> <th>area = large</th> <th>maxDets = 20</th> <th>0.255</th>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jAApuRJK7JxM"
   },
   "source": [
    "## Section 3: Applications\n",
    "Some applications in the field of deep learning have been developed in recent years with themes relating to the detection of the position of people in different scenes. Some of these include the classification of postures, detection of people in position, robots assisted living, character animation, video games industry, medical applications such as postural corrections, and anothers [interesting projects](https://medium.com/beyondminds/an-overview-of-human-pose-estimation-with-deep-learning-d49eb656739b). Below are some articles of interest:\n",
    "\n",
    "1.   [Multi-Person Pose Estimation for Pose Tracking with Enhanced Cascaded Pyramid Network](https://openaccess.thecvf.com/content_ECCVW_2018/papers/11130/Yu_Multi-Person_Pose_Estimation_for_Pose_Tracking_with_Enhanced_Cascaded_Pyramid_ECCVW_2018_paper.pdf)\n",
    "2.   [Single-Stage Multi-Person Pose Machines](https://arxiv.org/pdf/1908.09220.pdf)\n",
    "3.   [Rehabilitation Posture Correction Using Deep Neural\n",
    "Network](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7881743)\n",
    "4.   [Pose Trainer: Correcting Exercise Posture using Pose Estimation](https://arxiv.org/abs/2006.11718)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Ysg5djnUntE"
   },
   "source": [
    "In this notebook, we will attack in a particular aplication: The detection of multiple person poses in videos and their prediction in later frames. For this, we will use the [`tv_human_interactions`](https://www.robots.ox.ac.uk/~vgg/data/tv_human_interactions/) database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Wwp9Gc9Bg2Q"
   },
   "source": [
    "### Problem context\n",
    "\n",
    "We must understand that for every image in the video, we have that the network predicts an affinity map and a body part map, with sizes  $(46,83,38)$ and $(46,83,19)$, respectively. To predict the pose in future frames of a video we will use this information as an RCNN target, coupling each detection in a single tensor. The idea will be to be able to predict these two maps of a given frame a map of the current state, coupled into a single map set of $(46,83,57)$ size for a single image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eYn0nymvc1_E"
   },
   "source": [
    "### Download database\n",
    "\n",
    "As mentioned above, the database will be downloaded from [`tv_human_interactions`](https://www.robots.ox.ac.uk/~vgg/data/tv_human_interactions/). A state flag will allow you to switch between downloading and not downloading the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hR5UNP5Xdc2w"
   },
   "outputs": [],
   "source": [
    "download_tv_human = False\n",
    "if download_tv_human and not os.path.isdir(\"./data/tv_human_interactions_videos\"):\n",
    "    if not os.path.isdir(\"./data\"): os.mkdir(\"./data\")\n",
    "    !wget -P \"./data\" \"https://www.robots.ox.ac.uk/~vgg/data/tv_human_interactions/data/tv_human_interactions_videos.tar.gz\"\n",
    "    !tar -xzvf \"./data/tv_human_interactions_videos.tar.gz\" -C \"./data\"\n",
    "    os.remove(\"./data/tv_human_interactions_videos.tar.gz\")\n",
    "    output.clear()\n",
    "    print(\"[INFO] TV Human Interactions database download succesfully!.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hpcf2siNR6Qi"
   },
   "source": [
    "### Architecture implementation in videos\n",
    "Let's start with the implementation of the architecture in a test video. We will make the frame to frame reading, showing the result of the estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d9BuVeQxSEab"
   },
   "outputs": [],
   "source": [
    "''' Extract properties from video '''\n",
    "def video_prop_read(video_path, force_mp4 = True):\n",
    "    video = cv2.VideoCapture(video_path) # Read video\n",
    "    w,h = int(video.get(cv2.CAP_PROP_FRAME_WIDTH)), int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    video_fps = video.get(cv2.CAP_PROP_FPS)\n",
    "    codec = [chr((int(video.get(cv2.CAP_PROP_FOURCC)) >> 8 * i) & 0xFF) for i in range(4)]\n",
    "    video_size = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    video.release()\n",
    "    if \"mp4\" in os.path.splitext(video_path)[-1].lower() or force_mp4: codec = ['m', 'p', '4', 'v']\n",
    "    if video_size <= 0: video_size = np.inf\n",
    "    if video_fps <= 0 or video_fps == np.inf: video_fps = 25 # Default value\n",
    "    return w, h, video_fps, codec, video_size\n",
    "\n",
    "''' Show embed video'''\n",
    "def show_video(video_path):\n",
    "    filename, ext = os.path.splitext(video_path)\n",
    "    if os.path.isfile(video_path):\n",
    "        if \"mp4\" not in ext.lower():\n",
    "            filename += \".mp4\"\n",
    "            !sudo ffmpeg -t 5 -i \"$video_path\" \"$filename\" # Convert any ext to mp4\n",
    "            output.clear()\n",
    "            video_path = filename\n",
    "        video = Video.from_file(video_path)\n",
    "    else:\n",
    "        print(\"[ERROR] Video file not found. Please check path.\")\n",
    "        video = None\n",
    "    return video\n",
    "\n",
    "''' Draw humans in video '''\n",
    "def video_forward(video_in, fps = None, video_out_path = None, force_mp4 = True,\n",
    "                  total_frames = None, skip_frames = None, resize = None, \n",
    "                  background = True, print_im = False):\n",
    "    # Read video properties\n",
    "    video_w, video_h, video_fps, video_codec, video_size = video_prop_read(video_in, force_mp4)\n",
    "    video = cv2.VideoCapture(video_in)\n",
    "\n",
    "    # Video object to save and video_in read\n",
    "    if fps is not None and fps < video_fps: video_fps = fps\n",
    "    if video_out_path is not None:\n",
    "        video_out_path, ext = os.path.splitext(video_out_path)\n",
    "        if force_mp4: video_out_path += \".mp4\"\n",
    "        elif len(ext) == 0: video_out_path += os.path.splitext(video_in)[-1]\n",
    "        else: video_out_path += ext\n",
    "        if resize is not None: video_w, video_h = resize\n",
    "        video_out = cv2.VideoWriter(video_out_path, cv2.VideoWriter_fourcc(*video_codec), \n",
    "                                    video_fps, (video_w, video_h))\n",
    "        print(\"[INFO]: Video will be saved in\", video_out_path)\n",
    "\n",
    "    fcount = 0; tic = time.time(); total_paf = []; total_heat = []; sizes = []; rcount = 0\n",
    "    while video.isOpened():\n",
    "        ret, frame = video.read(); fcount += 1\n",
    "        if fps is not None and fcount % (fps//video_fps) != 0: continue # Skip frames\n",
    "        rcount += 1\n",
    "        if skip_frames is not None and rcount <= skip_frames : continue # Skip frames\n",
    "        if (cv2.waitKey(1) & 0xFF == ord('q')) or not ret: break # End of video\n",
    "\n",
    "        # Detection process\n",
    "        paf, heatmap, im_scale = im_forward(frame, rtpose_model) # CNN maps\n",
    "        total_paf += [paf]; total_heat += [heatmap]; sizes += [im_scale] # Concat detections\n",
    "\n",
    "        # Save pose-detection in video_out\n",
    "        if video_out_path is not None or print_im:\n",
    "            humans = paf_to_pose_cpp(heatmap, paf, cfg)\n",
    "            if not background: frame = np.zeros(frame.shape, dtype = \"uint8\")\n",
    "            frame_out = draw_humans(frame, humans)\n",
    "            if resize is not None: frame_out = cv2.resize(frame_out, resize)\n",
    "            if video_out_path is not None: video_out.write(frame_out)\n",
    "        \n",
    "        # if isprogrammer: cv2.imshow(\"output.mp4\", frame)\n",
    "        if fcount % 30 == 0:\n",
    "            print(\"[INFO]: {} of {} frames processed.\".format(fcount, video_size))\n",
    "            if print_im: \n",
    "                plt.imshow(cv2.cvtColor(frame_out, cv2.COLOR_BGR2RGB))\n",
    "                plt.axis(\"off\"); plt.show()\n",
    "        \n",
    "        if total_frames is not None and rcount >= total_frames: break # End\n",
    "        \n",
    "    video.release()\n",
    "    if video_out_path is not None: \n",
    "        video_out.release()\n",
    "        if \"mp4\" in os.path.splitext(video_out_path)[-1]:\n",
    "            video_out_path_compress = video_out_path.replace(\".mp4\",\"_out.mp4\")\n",
    "            !sudo ffmpeg -t 5 -i \"$video_out_path\" \"$video_out_path_compress\" # Compress\n",
    "            !mv \"$video_out_path_compress\" \"$video_out_path\"\n",
    "            !rm \"$video_out_path_compress\"\n",
    "            output.clear()\n",
    "        print(\"[INFO]: Video saved successfully\")\n",
    "    print(\"[INFO]: Total time spend in procedure:\", time.time() - tic, \"s\")\n",
    "    if len(total_paf) == 0: return None, None, None\n",
    "    else: return np.stack(total_paf, axis = 0), np.stack(total_heat, axis = 0), sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "AwoU7KeEaYr5",
    "outputId": "b4899a3d-b942-4538-d746-c4e4ab23ede1"
   },
   "outputs": [],
   "source": [
    "video_path_proof = \"./data/Videos_Deep_MiosHAHA/Entrenamiento/Original1_Train.mp4\"\n",
    "demo_pred = video_forward(video_path_proof, video_out_path = \"./demo/video_demo\", \n",
    "                          resize = (720,480), fps = 150, background = True, print_im = True)\n",
    "print(demo_pred[0].shape, demo_pred[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 505,
     "referenced_widgets": [
      "f0d67a9247ef4ecb95b30d15d88d5086",
      "fd5e116eadfc45ac93130eea2f3599c1"
     ]
    },
    "id": "-Rg8J-ACbpon",
    "outputId": "1b782d48-c037-4eb9-ce5e-52c01126a93f"
   },
   "outputs": [],
   "source": [
    "show_video(\"./demo/video_demo.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8_20lTfccOm"
   },
   "source": [
    "### Pose-database creation\n",
    "After we have tested the network in a simple video, we will split the video list files into test, train and validation sets.\n",
    "\n",
    "In addition, we will perform a mini-batch training, where each batche will be represented by each video. Therefore, we will define a function that will allow us to process each video and organize it in tensors, so that it organizes the information for the input/output training process. As the input is the composition of the prediction in $N$ previous states to predict the $N+1$ state (output), the dimenssions of the input tensor will be of $(x,N,46,83,57)$ and $(x,46,83,57)$ for the output, with $x$ the total of images in the video. If we desired $N=1$, $(x,46,83,57)$ will be the dimension of the entry tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BDO1QRgScbkV",
    "outputId": "369d6f49-2cf8-4f5a-cdc1-0cfa39021c89"
   },
   "outputs": [],
   "source": [
    "# Train, test and valid video lists creation\n",
    "random_state = 42\n",
    "video_list = readFileList(\"./data/tv_human_interactions_videos/\", ext = 'avi')\n",
    "videoloaders = {x:y for x,y in zip([\"train\",\"test\"], train_test_split(video_list, test_size = 0.3, random_state = random_state))}\n",
    "videoloaders.update({x:y for x,y in zip([\"valid\",\"test\"], train_test_split(videoloaders[\"test\"], test_size = 0.5, random_state = random_state))})\n",
    "video_set_sizes = {x: len(videoloaders[x]) for x in ['train', 'valid', 'test']}\n",
    "print(video_set_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sgtVyOhws1mJ",
    "outputId": "7de1cf96-fa2d-49d5-91c1-8b948b902ee4"
   },
   "outputs": [],
   "source": [
    "np.random.seed(random_state)\n",
    "videoloaders = {x:readFileList(\"./data/Videos_Deep_MiosHAHA/\" + y, ext = \"mp4\") for x,y in zip([\"train\",\"valid\",\"test\"],[\"Entrenamiento\",\"Validacion\",\"Test\"])}\n",
    "video_set_sizes = {x: len(videoloaders[x]) for x in ['train', 'valid', 'test']}\n",
    "print(video_set_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6t2KeflHEJ9I"
   },
   "outputs": [],
   "source": [
    "# Load (predict) video and split it in in/out tensor\n",
    "def video_batch(video_path, n_past_step = 12, n_fut_step = 12, \n",
    "                batch_size = None, skip_batch = None, **kwargs): # (b,t,c,h,w)\n",
    "    total_frames = batch_size; skip_frames = skip_batch\n",
    "    if skip_frames is not None: skip_frames *= n_past_step + n_fut_step\n",
    "    if batch_size is not None: \n",
    "        total_frames *= n_past_step + n_fut_step\n",
    "        if skip_frames is not None: total_frames += skip_frames\n",
    "        \n",
    "    tpaf, theat, _ = video_forward(video_path, # Video forward in RT-model (frames,h,w,c)\n",
    "            total_frames = total_frames, skip_frames = skip_frames, **kwargs)\n",
    "    if tpaf is None or theat is None: return None, None\n",
    "    paf_batch, heat_batch = [], []\n",
    "    for t in range(len(tpaf)//(n_past_step + n_fut_step)): # Batches of (B,n_steps,H,W,C)\n",
    "        paf_batch.append(tpaf[t*(n_past_step + n_fut_step):(t+1)*(n_past_step + n_fut_step)])\n",
    "        heat_batch.append(theat[t*(n_past_step + n_fut_step):(t+1)*(n_past_step + n_fut_step)])\n",
    "    if len(heat_batch) == 0 or len(paf_batch) == 0: return None, None\n",
    "    paf_batch = torch.from_numpy(np.stack(paf_batch, axis = 0)).float()\n",
    "    heat_batch = torch.from_numpy(np.stack(heat_batch, axis = 0)).float()\n",
    "    return paf_batch.permute(0,1,4,2,3), heat_batch.permute(0,1,4,2,3) # Tensors organize to (B,past+future,C,H,W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_IFqXrABMBmT"
   },
   "source": [
    "### RCNN architecture definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LI692SOeSJfN"
   },
   "outputs": [],
   "source": [
    "''' Defined a single convLSTM module '''\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\n",
    "        \"\"\"\n",
    "        Initialize ConvLSTM cell.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim: int # Number of channels of input tensor.\n",
    "        hidden_dim: int # Number of channels of hidden state.\n",
    "        kernel_size: (int, int) # Size of the convolutional kernel.\n",
    "        bias: bool # Whether or not to add the bias.\n",
    "        \"\"\"\n",
    "\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
    "        self.bias = bias\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels = self.input_dim + self.hidden_dim,\n",
    "                              out_channels = 4 * self.hidden_dim,\n",
    "                              kernel_size = self.kernel_size,\n",
    "                              padding = self.padding, bias = self.bias)\n",
    "\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state\n",
    "\n",
    "        # concatenate along channel axis\n",
    "        combined = torch.cat([input_tensor, h_cur], dim = 1) # (B,c,h,w)\n",
    "\n",
    "        combined_conv = self.conv(combined)\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim = 1)\n",
    "        i = torch.sigmoid(cc_i) # (B, hidden, h, w)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        height, width = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device),\n",
    "                torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LkMZRCoESYfm"
   },
   "outputs": [],
   "source": [
    "class EncoderDecoderConvLSTM(nn.Module):\n",
    "    def __init__(self, nf, in_chan):\n",
    "        super(EncoderDecoderConvLSTM, self).__init__()\n",
    "\n",
    "        \"\"\" ARCHITECTURE \n",
    "\n",
    "        # Encoder (ConvLSTM)\n",
    "        # Encoder Vector (final hidden state of encoder)\n",
    "        # Decoder (ConvLSTM) - takes Encoder Vector as input\n",
    "        # Decoder (3D CNN) - produces regression predictions for our model\n",
    "\n",
    "        \"\"\"\n",
    "        # PAF predictions\n",
    "        self.encoder_1_convlstm = ConvLSTMCell(input_dim=in_chan[0], hidden_dim=nf, kernel_size=(3, 3), bias=True)\n",
    "        self.decoder_1_convlstm = ConvLSTMCell(input_dim=nf, hidden_dim=nf, kernel_size=(3, 3), bias=True)\n",
    "        self.decoder_1_CNN = nn.Conv3d(in_channels=nf, out_channels=in_chan[0], kernel_size=(1, 3, 3), padding=(0, 1, 1))\n",
    "\n",
    "        # Heatmap predictions\n",
    "        self.encoder_2_convlstm = ConvLSTMCell(input_dim=in_chan[1], hidden_dim=nf, kernel_size=(3, 3), bias=True)\n",
    "        self.decoder_2_convlstm = ConvLSTMCell(input_dim=nf, hidden_dim=nf, kernel_size=(3, 3), bias=True)\n",
    "        self.decoder_2_CNN = nn.Conv3d(in_channels=nf, out_channels=in_chan[1], kernel_size=(1, 3, 3), padding=(0, 1, 1))\n",
    "\n",
    "    def autoencoder(self, x, seq_len, future_step, h_t, c_t, h_t2, c_t2, h_t3, c_t3, h_t4, c_t4):\n",
    "        # encoder\n",
    "        for t in range(seq_len):\n",
    "            h_t, c_t = self.encoder_1_convlstm(input_tensor = x[0][:, t, :, :, :], cur_state=[h_t, c_t])\n",
    "            h_t2, c_t2 = self.encoder_2_convlstm(input_tensor = x[1][:, t, :, :, :], cur_state=[h_t2, c_t2])\n",
    "        \n",
    "        encoder_vector = [h_t, h_t2] # (B, Hidden, H, W)\n",
    "\n",
    "        # decoder\n",
    "        paf_outputs, heat_outputs = [], []\n",
    "        for t in range(future_step):\n",
    "            h_t3, c_t3 = self.decoder_1_convlstm(input_tensor=encoder_vector[0], cur_state=[h_t3, c_t3])\n",
    "            h_t4, c_t4 = self.decoder_2_convlstm(input_tensor=encoder_vector[1], cur_state=[h_t4, c_t4])\n",
    "            encoder_vector = [h_t3, h_t4] # (B,hidden,H,W)\n",
    "            paf_outputs += [h_t3]; heat_outputs += [h_t4] # predictions\n",
    "\n",
    "        # PAF prediction\n",
    "        paf_outputs = torch.stack(paf_outputs, dim = 1) # (B,future_step,hidden,H,W)\n",
    "        paf_outputs = paf_outputs.permute(0, 2, 1, 3, 4) # (B,hidden,future_step,H,W)\n",
    "        paf_outputs = self.decoder_1_CNN(paf_outputs) # (B,C,future_step,H,W)\n",
    "        paf_outputs = torch.nn.Tanh()(paf_outputs)\n",
    "        paf_outputs = paf_outputs.permute(0, 2, 1, 3, 4) # (B,future_step,C,H,W)\n",
    "\n",
    "        # Heat predictions\n",
    "        heat_outputs = torch.stack(heat_outputs, dim = 1) # (B,future_step,hidden,H,W)\n",
    "        heat_outputs = heat_outputs.permute(0, 2, 1, 3, 4) # (B,hidden,future_step,H,W)\n",
    "        heat_outputs = self.decoder_2_CNN(heat_outputs) # (B,C,future_step,H,W)\n",
    "        heat_outputs = torch.nn.Tanh()(heat_outputs)\n",
    "        heat_outputs = heat_outputs.permute(0, 2, 1, 3, 4) # (B,future_step,C,H,W)\n",
    "        return paf_outputs, heat_outputs\n",
    "    \n",
    "    def forward(self, x, n_step_fut, hidden_state = None):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_tensor:\n",
    "            List with two 5-D Tensor of shape (b, t, c, h, w) # batch, time, channel, height, width\n",
    "        \"\"\"\n",
    "\n",
    "        # find size of different input dimensions\n",
    "        b, seq_len, _, h, w = x[0].size()\n",
    "\n",
    "        # initialize hidden states\n",
    "        if hidden_state is None:\n",
    "            h_t, c_t = self.encoder_1_convlstm.init_hidden(batch_size=b, image_size=(h, w))\n",
    "            h_t2, c_t2 = self.encoder_2_convlstm.init_hidden(batch_size=b, image_size=(h, w))\n",
    "            h_t3, c_t3 = self.decoder_1_convlstm.init_hidden(batch_size=b, image_size=(h, w))\n",
    "            h_t4, c_t4 = self.decoder_2_convlstm.init_hidden(batch_size=b, image_size=(h, w))\n",
    "        else:\n",
    "            (h_t, c_t), (h_t2, c_t2), (h_t3, c_t3), (h_t4, c_t4) = hidden_state\n",
    "\n",
    "        # autoencoder forward\n",
    "        outputs = self.autoencoder(x, seq_len, n_step_fut, h_t, c_t, h_t2, c_t2, h_t3, c_t3, h_t4, c_t4)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEpx0dAMTUfh"
   },
   "source": [
    "### Training model definition\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M4nkUuEkTTvG"
   },
   "outputs": [],
   "source": [
    "def train_step(model, criterion, optimizer = None, phase = 'valid', **kwargs):\n",
    "    if phase == 'train': model.train()  # Set model to training mode\n",
    "    else: model.eval() # Set model to evaluate mode\n",
    "\n",
    "    skip_batch, losses, batch_count = 0, 0.0, 0\n",
    "    while True:\n",
    "        paf_batch, heat_batch = video_batch(skip_batch = skip_batch, **kwargs) # Batch of (B,frames,C,H,W)\n",
    "        if paf_batch is None or heat_batch is None: break # Invalid video read.\n",
    "        batch_count += 1\n",
    "\n",
    "        # Batch I/O split\n",
    "        x_paf = paf_batch[:, :kwargs[\"n_past_step\"]].cuda() # Batch of (B,n_steps,C,H,W)\n",
    "        y_paf = paf_batch[:, kwargs[\"n_past_step\"]:].cuda() # Batch of (B,future_step,C,H,W)\n",
    "        x_heat = heat_batch[:, :kwargs[\"n_past_step\"]].cuda() # Batch of (B,n_steps,C,H,W)\n",
    "        y_heat = heat_batch[:, kwargs[\"n_past_step\"]:].cuda() # Batch of (B,future_step,C,H,W)\n",
    "\n",
    "        # forward\n",
    "        with torch.set_grad_enabled(phase == 'train'): # track history if only in train\n",
    "            y_hat1, y_hat2 = model([x_paf, x_heat], n_step_fut = kwargs[\"n_fut_step\"])\n",
    "            loss = criterion(y_hat1, y_paf) + criterion(y_hat2, y_heat)\n",
    "            if phase == 'train': # backward + optimize only if in training phase\n",
    "                optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "\n",
    "        losses += loss.item() * x_paf.size(0) # Total images\n",
    "        \n",
    "        if kwargs[\"batch_size\"] is not None: skip_batch += kwargs[\"batch_size\"]\n",
    "        else: break # Invalid video read.\n",
    "    if batch_count == 0: return None\n",
    "    else: return losses/batch_count\n",
    "\n",
    "def load_stats(model, stat_name, model_name):\n",
    "    if stat_name is not None and os.path.isfile(stat_name):\n",
    "        with open(stat_name, \"r\") as f: stats = json.load(f) # Recovery stats\n",
    "        if model_name is not None: stats[\"model_name\"] = model_name\n",
    "    else:\n",
    "        stats = {\"losses\": {\"train\": [], \"valid\": []}, \"cur_epoch\":0, # Variable to save all\n",
    "                \"best_epoch\": 0, \"best_loss\": np.inf, \"time_process\": 0,\n",
    "                 \"model_name\": model_name}\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(stats[\"model_name\"]))\n",
    "        model.cuda()\n",
    "    except:\n",
    "        pass\n",
    "    return model, stats\n",
    "\n",
    "def training(EDconvLST_model, criterion, optimizer, scheduler = None, num_epochs = 25, \n",
    "             model_name = None, early_max = None, n_steps_past = 10, future_step = 10, \n",
    "             stat_name = None, batch_size = None):\n",
    "    \n",
    "    since = time.time(); count_max = 0; tic = since\n",
    "\n",
    "    # Model initialization\n",
    "    EDconvLST_model, stats = load_stats(EDconvLST_model, stat_name, model_name)\n",
    "    t0 = stats[\"time_process\"]\n",
    "    if stats[\"cur_epoch\"] >= num_epochs: \n",
    "        print(\"[INFO] Model already was training. Return (if exist) the train model\")\n",
    "        if stats[\"model_name\"] is None: EDconvLST_model = None\n",
    "        return EDconvLST_model, stats\n",
    "    \n",
    "    # Train process\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        # torch.cuda.empty_cache()\n",
    "        if epoch <= stats[\"cur_epoch\"]: continue # Skip epochs\n",
    "        isprint = num_epochs <= 1000 or (epoch-1) % (num_epochs // 10) == 0 or epoch == num_epochs\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'valid']:\n",
    "            # Iterate over data.\n",
    "            running_loss = 0.0\n",
    "            np.random.shuffle(videoloaders[phase])\n",
    "            for i,video_path in enumerate(videoloaders[phase]): # Start for\n",
    "                if isprint: print(\"[INFO] {}: Reading video {}/{}\".format(phase, i, video_set_sizes[phase]))\n",
    "                \n",
    "                loss = train_step(EDconvLST_model, criterion, optimizer = optimizer, phase = phase, \n",
    "                                  video_path = video_path, n_past_step = n_steps_past, n_fut_step = future_step, \n",
    "                                  batch_size = batch_size, resize = (720,480), fps = 150)\n",
    "                \n",
    "                if isprint: \n",
    "                    output.clear()\n",
    "                    print('\\nEpoch {}/{}\\n{}'.format(epoch, num_epochs, '-'*15))\n",
    "                    if len(stats[\"losses\"][\"train\"]) > 0: train_loss = stats[\"losses\"][\"train\"][-1]\n",
    "                    else: train_loss = np.inf\n",
    "                    if loss is not None:\n",
    "                        print('[INFO] {}: Current loss = {:.6f}, last train loss = {:.6f}, best valid loss = {:.6f}'.\\\n",
    "                          format(phase, loss, train_loss, stats[\"best_loss\"]))\n",
    "                    else:\n",
    "                        print('[INFO] {}: Last train loss = {:.6f}, best valid loss = {:.6f}'.\\\n",
    "                          format(phase, train_loss, stats[\"best_loss\"]))\n",
    "            \n",
    "                if loss is not None: running_loss += loss\n",
    "            if isprint: output.clear() # Errase print info\n",
    "            if phase == 'train' and scheduler is not None: scheduler.step()\n",
    "            stats[\"losses\"][phase].append(running_loss/video_set_sizes[phase])\n",
    "        \n",
    "        time_elapsed = time.time() - tic\n",
    "        if isprint: # Print somethings for some epoch\n",
    "            print('train Loss: {:.6f} \\tvalid Loss: {:.6f}. Spend time:{:.0f}m {:.0f}s'\\\n",
    "                .format(stats[\"losses\"][\"train\"][-1], stats[\"losses\"][\"valid\"][-1], time_elapsed // 60, time_elapsed % 60))\n",
    "            tic = time.time() # For next iteration\n",
    "\n",
    "        # deep save model\n",
    "        if stats[\"losses\"][\"valid\"][-1] < stats[\"best_loss\"]:\n",
    "            if isprint: \n",
    "                print('Valid loss decreased ({:.6f} --> {:.6f}). Saving model ...'.format(stats[\"best_loss\"], stats[\"losses\"][\"valid\"][-1]))\n",
    "            stats[\"best_epoch\"] = epoch\n",
    "            stats[\"best_loss\"] = stats[\"losses\"][\"valid\"][-1]\n",
    "            best_model_wts = copy.deepcopy(EDconvLST_model.state_dict())\n",
    "            if model_name is not None: torch.save(best_model_wts, model_name)\n",
    "            count_max = 0\n",
    "        else: \n",
    "            count_max += 1 \n",
    "            if early_max is not None and count_max >= early_max: break # Early finish training\n",
    "\n",
    "        stats[\"time_process\"] = time.time() - since + t0\n",
    "        stats[\"cur_epoch\"] = epoch\n",
    "        if stat_name is not None: \n",
    "            with open(stat_name, \"w\") as f: json.dump(stats, f)\n",
    "\n",
    "    print('\\n{}\\nTraining complete ({}/{} epochs) in {:.0f}m {:.0f}s'\\\n",
    "          .format('-'*25, epoch, num_epochs, stats[\"time_process\"] // 60, stats[\"time_process\"] % 60))\n",
    "    print('Best metric (loss): {:4f}'.format(stats[\"best_loss\"]))\n",
    "\n",
    "    # load best model weights\n",
    "    EDconvLST_model.load_state_dict(best_model_wts)\n",
    "    EDconvLST_model.cuda()\n",
    "    return EDconvLST_model, stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28xlkBcmbEWQ"
   },
   "source": [
    "### User parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ToOGoqWG_Oau"
   },
   "outputs": [],
   "source": [
    "# User parameters\n",
    "n_steps_past = 10\n",
    "n_steps_fut = 1\n",
    "num_epochs = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eG-HettwbD1S"
   },
   "outputs": [],
   "source": [
    "EDCLSTM_model = EncoderDecoderConvLSTM(nf = 128, in_chan = [38,19])\n",
    "EDCLSTM_model.cuda()\n",
    "criterion = nn.MSELoss(reduction = \"sum\").cuda()\n",
    "optimizer = optim.Adam(EDCLSTM_model.parameters(), lr = 1e-1, betas=(0.9, 0.98))\n",
    "# optimizer = optim.SGD(EDCLSTM_model.parameters(), lr = 1e-2)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 50, gamma = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "id": "gZfaZkBhcl08",
    "outputId": "d20becd4-d6d5-4626-d070-01ebc3b72bf6"
   },
   "outputs": [],
   "source": [
    "model_file = \"EDCLSTM_exp3.pt\"\n",
    "statistics_file = \"EDCLSTM_exp3.json\"\n",
    "EDCLSTM_model, EDCLSTM_stats = training(EDCLSTM_model, criterion, optimizer, batch_size = None,\n",
    "             num_epochs = num_epochs, early_max = None, n_steps_past = n_steps_past, scheduler = scheduler,\n",
    "             future_step = n_steps_fut, stat_name = statistics_file, model_name = model_file)\n",
    "# EDCLSTM_model, EDCLSTM_stats = load_stats(EDCLSTM_model, statistics_file, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "01px-RNbmc5T"
   },
   "outputs": [],
   "source": [
    "def loss_plot(losses):\n",
    "    for phase in [\"train\", \"valid\"]:\n",
    "        plt.plot(range(1, len(losses[phase]) + 1), losses[phase], label = phase + \" losses\")\n",
    "    plt.legend(prop = {'size': 10})\n",
    "    plt.title('loss function', size = 10)\n",
    "    plt.xlabel('epoch', size = 10); plt.ylabel('loss value', size = 10)\n",
    "\n",
    "loss_plot(EDCLSTM_stats[\"losses\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-yBJgw9A-nIm"
   },
   "outputs": [],
   "source": [
    "def test_metrics(model, video_list, criterion, np, nf): # Metricas\n",
    "    running_loss = 0.0\n",
    "    for video_path in video_list: # Start for\n",
    "        loss = train_step(model, criterion, video_path = video_path, \n",
    "                          n_past_step = np, n_fut_step = nf, \n",
    "                          batch_size = None, resize = (720,480), fps = 150)\n",
    "        if loss is not None: running_loss += loss\n",
    "    running_loss /= len(video_list)\n",
    "    output.clear()\n",
    "    print(\"[INFO] Loss in dataset:\", running_loss)\n",
    "\n",
    "test_metrics(EDCLSTM_model, videoloaders[\"test\"], criterion, np = n_steps_past, nf = n_steps_fut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bH8nYS8ZApvD"
   },
   "outputs": [],
   "source": [
    "''' Draw humans in video  with detection'''\n",
    "def video_prediction(model, video_in, step_past, fps = None, video_out_path = None, force_mp4 = True,\n",
    "                  total_frames = None, skip_frames = None, resize = None, \n",
    "                  background = True, print_im = False):\n",
    "    # Read video properties\n",
    "    video_w, video_h, video_fps, video_codec, video_size = video_prop_read(video_in, force_mp4)\n",
    "    video = cv2.VideoCapture(video_in)\n",
    "\n",
    "    # Video object to save and video_in read\n",
    "    if fps is not None and fps < video_fps: video_fps = fps\n",
    "    if video_out_path is not None:\n",
    "        video_out_path, ext = os.path.splitext(video_out_path)\n",
    "        if force_mp4: video_out_path += \".mp4\"\n",
    "        elif len(ext) == 0: video_out_path += os.path.splitext(video_in)[-1]\n",
    "        else: video_out_path += ext\n",
    "        if resize is not None: video_w, video_h = resize\n",
    "        video_out = cv2.VideoWriter(video_out_path, cv2.VideoWriter_fourcc(*video_codec), \n",
    "                                    video_fps, (video_w, video_h))\n",
    "        print(\"[INFO]: Video will be saved in\", video_out_path)\n",
    "\n",
    "    fcount = 0; tic = time.time(); total_paf = []; total_heat = []; rcount = 0\n",
    "    model.eval()\n",
    "    while video.isOpened():\n",
    "        ret, frame = video.read(); fcount += 1\n",
    "        if fps is not None and fcount % (fps//video_fps) != 0: continue # Skip frames\n",
    "        rcount += 1\n",
    "        if skip_frames is not None and rcount <= skip_frames : continue # Skip frames\n",
    "        if (cv2.waitKey(1) & 0xFF == ord('q')) or not ret: break # End of video\n",
    "\n",
    "        # Detection process\n",
    "        if len(total_paf) < step_past:\n",
    "            paf, heatmap, _ = im_forward(frame, rtpose_model) # CNN maps\n",
    "            total_paf += [paf]; total_heat += [heatmap]; # Concat detections\n",
    "            # plt.hist(paf.flatten(), bins = 12); plt.hist(heatmap.flatten(), bins = 12); \n",
    "            # plt.show()\n",
    "        else:\n",
    "            paf_i, heatmap_i, _ = im_forward(frame, rtpose_model) # CNN maps\n",
    "            with torch.set_grad_enabled(False):\n",
    "                x_paf = torch.from_numpy(np.stack(total_paf, axis = 0)[None]).cuda().permute(0,1,4,2,3) # to (B,sp,C,H,W)\n",
    "                x_heat = torch.from_numpy(np.stack(total_heat, axis = 0)[None]).cuda().permute(0,1,4,2,3) # to (B,sp,C,H,W)\n",
    "                paf, heatmap = model([x_paf, x_heat], n_step_fut = 1) # Only one future prediction\n",
    "                paf = paf.squeeze().permute(1,2,0).cpu().detach().numpy() # Size of (H,W,C)\n",
    "                heatmap = heatmap.squeeze().permute(1,2,0).cpu().detach().numpy() # Size of (H,W,C)\n",
    "            total_paf += [paf]; total_heat += [heatmap]; # Concat detections\n",
    "            # total_paf = total_paf[-step_past:]; total_heat = total_heat[-step_past:]\n",
    "            mse = (np.square(paf - paf)).sum(axis=None)\n",
    "            print(\"[INFO]: MSE = \", mse)\n",
    "        \n",
    "        # Save pose-detection in video_out\n",
    "        if video_out_path is not None or print_im:\n",
    "            humans = paf_to_pose_cpp(heatmap, paf, cfg)\n",
    "            if not background: frame = np.zeros(frame.shape, dtype = \"uint8\")\n",
    "            frame_out = draw_humans(frame, humans)\n",
    "            if resize is not None: frame_out = cv2.resize(frame_out, resize)\n",
    "            if video_out_path is not None: video_out.write(frame_out)\n",
    "        \n",
    "        # if isprogrammer: cv2.imshow(\"output.mp4\", frame)\n",
    "        if fcount % 1 == 0:\n",
    "            print(\"[INFO]: {} of {} frames processed.\".format(fcount, video_size))\n",
    "            if print_im: \n",
    "                plt.imshow(cv2.cvtColor(frame_out, cv2.COLOR_BGR2RGB))\n",
    "                plt.axis(\"off\"); plt.show()\n",
    "        \n",
    "        if total_frames is not None and rcount >= total_frames: break # End\n",
    "        \n",
    "    video.release()\n",
    "    if video_out_path is not None: \n",
    "        video_out.release()\n",
    "        if \"mp4\" in os.path.splitext(video_out_path)[-1]:\n",
    "            video_out_path_compress = video_out_path.replace(\".mp4\",\"_out.mp4\")\n",
    "            !sudo ffmpeg -t 5 -i \"$video_out_path\" \"$video_out_path_compress\" # Compress\n",
    "            !mv \"$video_out_path_compress\" \"$video_out_path\"\n",
    "            !rm \"$video_out_path_compress\"\n",
    "            output.clear()\n",
    "        print(\"[INFO]: Video saved successfully\")\n",
    "    print(\"[INFO]: Total time spend in procedure:\", time.time() - tic, \"s\")\n",
    "    if len(total_paf) == 0: return None, None\n",
    "    else: return np.stack(total_paf, axis = 0), np.stack(total_heat, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u1HXMGZYGeGM"
   },
   "outputs": [],
   "source": [
    "video_path_proof = \"./data/Videos_Deep_MiosHAHA/Entrenamiento/Original1_Train.mp4\"\n",
    "demo_pred2 = video_prediction(EDCLSTM_model, video_path_proof, step_past = n_steps_past, \n",
    "                              resize = (720,480), fps = 150, background = True, skip_frames = 6,\n",
    "                              print_im = True)\n",
    "print(demo_pred2[0].shape, demo_pred2[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GuGuIP6j7dc9"
   },
   "source": [
    "## ***Anexos: GitHub connection***\n",
    "Here, some functions to upload the github respository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XM88fGToxGPB"
   },
   "outputs": [],
   "source": [
    "''' Function definitions'''\n",
    "# Git pull\n",
    "def git_pull(repo_pwd, show_current_branch = False, make_commit = False): # Only for colab space work\n",
    "    global user_git, email_git\n",
    "    import sys\n",
    "    IN_COLAB = 'google.colab' in sys.modules\n",
    "    if IN_COLAB:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/gdrive')\n",
    "\n",
    "        %cd \"$repo_pwd\"\n",
    "        # !git config --list\n",
    "        if show_current_branch: \n",
    "            !git branch \n",
    "        if make_commit:\n",
    "            if \"user_git\" not in globals(): user_git = input(\"User github?: \")\n",
    "            if \"email_git\" not in globals(): email_git = input(\"Email github?: \") \n",
    "            !git config --global user.email \"$email_git\"\n",
    "            !git config --global user.name \"$user_git\"\n",
    "            !git commit -am \"Updating in colab\"\n",
    "        !git pull\n",
    "        !git status\n",
    "    else:\n",
    "        print(\"[INFO] You are not in collaboration, nothing has been done.\")\n",
    "\n",
    "# Git push\n",
    "def git_push(repo_pwd): # Only for colab space work\n",
    "    global user_git, email_git\n",
    "    import sys\n",
    "    IN_COLAB = 'google.colab' in sys.modules\n",
    "    if IN_COLAB:\n",
    "        from google.colab import drive\n",
    "        import getpass\n",
    "        drive.mount('/content/gdrive')\n",
    "\n",
    "        %cd \"$repo_pwd\"\n",
    "        if \"user_git\" not in globals(): user_git = input(\"User github?: \")\n",
    "        if \"email_git\" not in globals(): email_git = input(\"Email github?: \")\n",
    "\n",
    "        # Password login\n",
    "        try: \n",
    "            pwd_git = getpass.getpass(prompt='{} github password: '.format(user_git)) \n",
    "        except Exception as error: \n",
    "            print('ERROR', error) \n",
    "\n",
    "        # Upload from every where\n",
    "        origin_git = !git config --get remote.origin.url\n",
    "        origin_git = origin_git[0].replace(\"https://\",\"https://{}:{}@\".format(user_git,pwd_git))\n",
    "\n",
    "        !git config --global user.email \"$email_git\"\n",
    "        !git config --global user.name \"$user_git\"\n",
    "        !git status\n",
    "\n",
    "        x = \" \"\n",
    "        while x.lower() != \"y\" and x.lower() != \"n\": x = input(\"Continue?...[y/n]: \")\n",
    "\n",
    "        if x.lower() == \"y\":\n",
    "            com_message = input(\"Enter the commit message: \")\n",
    "            !git add .\n",
    "            !git commit -am \"$com_message\"\n",
    "            !git push \"$origin\"\n",
    "            !git status\n",
    "    else:\n",
    "        print(\"[INFO] You are not in collaboration, nothing has been done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wOTNnyQsDfXb"
   },
   "outputs": [],
   "source": [
    "repo_pwd = \"/content/gdrive/My Drive/Colab Notebooks/RT-multiperson-pose-pytorch\"\n",
    "# git_pull(repo_pwd, show_current_branch = False, make_commit = True)\n",
    "# git_push(repo_pwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GY2pPGvf8bad"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "pose_results.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "f0d67a9247ef4ecb95b30d15d88d5086": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "VideoModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VideoModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VideoView",
      "autoplay": true,
      "controls": true,
      "format": "mp4",
      "height": "",
      "layout": "IPY_MODEL_fd5e116eadfc45ac93130eea2f3599c1",
      "loop": true,
      "width": ""
     }
    },
    "fd5e116eadfc45ac93130eea2f3599c1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
